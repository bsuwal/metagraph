<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>Your Title - research</title>
 <link href="http://example.com/tag/research/index.xml" rel="self"/>
 <link href="http://example.com/tag/research.html"/>
 <updated>2017-05-25T13:31:27-04:00</updated>
 <id>http://example.com/tag/research.html</id>
 <author>
   <name>Author Here</name>
 </author>
 
 <entry>
   <title>Graphs are PAC-Learnable from Subgraphs</title>
   <link href="http://example.com/blog/2017/05/graphs-pac-learn.html"/>
   <updated>2017-05-23T00:00:00-04:00</updated>
   <id>http://example.com/blog/2017/05/graphs-pac-learn</id>
   <content type="html">&lt;p&gt;This semester I took a course on Computational Learning Theory, which deals with the statistical and computational underpinnings of machine learning.  As part of our final project, Hadi Elzayn and I proved that graphs are Probabily-Approximately Correct (PAC)-learnable from labeled supergraph/subgraph examples in polynomial sample complexity.  Here’s a presentation of that proof as well as a fun little way of trying out $\TeX$ on the blog.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;positive-examples-are-supergraphs&quot;&gt;Positive Examples are Supergraphs&lt;/h2&gt;

&lt;p&gt;We first consider a setting in which the Learner wishes to learn a graph $G$  on some vertex set $V=\{v_1,v_2,\dots,v_n\}$.  That is, the number of vertices is initially known, but the edges are initially unknown.  Observe that if $n$ is the number of vertices in the graph, our concept class contains $O(2^n)$ possible graphs, as we have each of the $\binom{n}{2} =\frac{n^2-n}{2}$ edges either present or absent in the true graph $G$.  Let $\mathcal{D}$ be some arbitrary distribution over all possible graphs on the vertex set $V$.  Let $EX(c,\mathcal{D})$ be a sampling oracle which returns a graph $G’$ and a label $y$, where $y=1$ if $G’$ is a supergraph of $G$ and $y=0$ if $G’$ is not a supergraph of $G$.  In other words, $y$ represents the answer to the question ‘Does $G’$ have at least every edge that is in $G$, and maybe some extras?’  We claim that $G$ is $(\epsilon,\delta)-$PAC learnable from $EX(c,\mathcal{D})$ with sufficiently many examples $m$, and that $m$ is only polynomial in $\epsilon$, $\delta$, and the number of vertices in the graph $n$.&lt;/p&gt;

&lt;p&gt;Trying to learn from supergraphs might feel a little contrived, and admittedly it is.  It might be a more natural question of whether $G$ can be PAC-learned from examples which are its subgraphs.  We start with supergraphs because there is an elegant and straightforward reduction from a problem that is known to be PAC-learnable to the supergraph case, and after showing this correspondence, the claim that $G$ is learnable from subgraphs under the same conditions will follow easily.&lt;/p&gt;

&lt;div class=&quot;theorem&quot;&gt;
Given error parameter $\epsilon$, confidence parameter $\delta$, a vertex set $V$, and an example oracle $EX(c,\mathcal{D})$, we can PAC-learn a graph $G$ from supergraphs using only positive examples with sample complexity polynomial in $\frac{1}{\epsilon},\frac{1}{\delta},$ and the number of vertices in $G$.
&lt;/div&gt;

&lt;p&gt;At a high level, our algorithm will start with the hypothesis $h=K^n$, the complete graph on $n$ vertices. That is, we assume that $G$ contains every possible edge. When we see a positive example $x$ from $EX(c,\mathcal{D})$, we delete from $h$ those edges not present in $x$.  We do this sufficiently many times to ensure we have error less than $\epsilon$ with probability at least $1-\delta$.&lt;/p&gt;

&lt;div class=&quot;lemma&quot;&gt;
We never delete from $h$ any edge actually present in the true $G$.
&lt;/div&gt;

&lt;p&gt;To see this, consider the condition for deleting an edge.  We only delete an edge if we see a positive example $x$ where that edge is not present.  Since the example is positive, it contains at least all of the edges that $G$ does, so $G$ certainly cannot contain any edge &lt;em&gt;not&lt;/em&gt; present in $x$.  We therefore know that our hypothesis $h$ will always be a supergraph of the true graph $G$.&lt;/p&gt;

&lt;p&gt;This is starting to feel a little bit like learning a monotone boolean conjunction.  We will show at the end of this section the explicit reduction which demonstrates that this is &lt;em&gt;exactly equivalent&lt;/em&gt; to learning a monotone boolean conjunction, but for now, we proceed with the proof.&lt;/p&gt;

&lt;p&gt;Let $e$ be some possible edge, and denote $p(e)$ the probability that $e$ gets deleted from $h$ as a result of seeing some positive example from $EX(c,\mathcal{D})$.  Note that for all $e$ in the true graph $G$, $p(e)=0$, as we showed above that we will never remove such an edge from $h$.&lt;/p&gt;

&lt;div class=&quot;lemma&quot;&gt;
The error of $h$, that is the probability that $h$ contains at least one edge not present in $G$ (equivalently, the probability that $h\neq G$) is upper-bounded by the sum of the $p(e)$ over all possible edges.
&lt;/div&gt;

&lt;p&gt;To see this, consider an edge $e$ which is in $h$.  Suppose $EX(c,\mathcal{D})$ gives us a positive example in which $e$ is not present. Since $e$ is in $h$ but is not in $G$, $h$ makes an error on that edge $e$.  We call $p(e)$ the probability that $e$ causes $h$ to make such an error, so $\sum\limits_{e\in G}p(e)$ must be an upper-bound on the error of $h$.&lt;/p&gt;

&lt;p&gt;Call an edge &lt;em&gt;bad&lt;/em&gt; if $p(e)\geq \frac{e}{|E|}$.&lt;/p&gt;

&lt;div class=&quot;lemma&quot;&gt;
If $h$ has no bad edges, then the error of $h$ is less than $\epsilon$, as desired.
&lt;/div&gt;

&lt;p&gt;This follows directly from the previous Lemma.&lt;/p&gt;

&lt;p&gt;We now need to show that we can achieve such a low error hypothesis with high probability.  What is the probability that a bad edge survives $m$ positive examples?  The probability that it survives any one draw is&lt;/p&gt;

&lt;p&gt;$(1-p(e))\leq (1-\frac{\epsilon}{|E|})$&lt;/p&gt;

&lt;p&gt;Then the probability that it survives $m$ draws is at most $(1-\frac{\epsilon}{|E|})^m$&lt;/p&gt;

&lt;p&gt;In the worst case, every one of the $|E|$ edges is bad, so the probability that any bad edge survives $m$ draws is at most $|E|(1-\frac{\epsilon}{|E|})^m$.  We want to choose $m$ large enough to make this smaller than $\delta$.  Using Hoeffding’s Inequality and some basic algebra, we get&lt;/p&gt;

&lt;p&gt;$m\geq \frac{|E|}{\epsilon}(\ln{|E|}+\ln{\frac{1}{\delta}})$&lt;/p&gt;

&lt;p&gt;We have $m$ polynomial in $|E|$ (which is itself polynomial in the number of vertices in $G$), $\delta$, and $\epsilon$, hence graphs are efficiently PAC-learnable from supergraphs. $\square$&lt;/p&gt;

&lt;h2 id=&quot;the-same-as-conjunctions&quot;&gt;The Same As Conjunctions&lt;/h2&gt;

&lt;p&gt;This proof almost exactly followed the proof in Kearns and Vazirani of boolean conjuctions being efficiently PAC-learnable.  In fact, everything from the structure to the sample complexity bound carries through because learning a graph from supergraphs is equivalent to learning a boolean conjuction from positive examples.&lt;/p&gt;

&lt;p&gt;Let $e_{ij}$ be a possible edge in $G$, and let $x_{ij}$ be the boolean variable which takes on the value $1$ (or true) when $e_{ij}$ is actually an edge in $G$ and $0$ (or false) otherwise.  Then a graph is just a conjuction over the positive instances of the literals corresponding to edges in the graph.  Deleting edges when they do not appear in a supergraph is equivalent to deleting literals which are $0$ in a positive example.&lt;/p&gt;

&lt;p&gt;From here, it should be simple to see how we can learn a graph $G$ from positive examples which are subgraphs of $G$, simply by considering the negation of the previous problem.&lt;/p&gt;

&lt;h2 id=&quot;positive-examples-are-subgraphs&quot;&gt;Positive Examples are Subgraphs&lt;/h2&gt;

&lt;p&gt;In this version of the problem we begin with $h$ as the graph on $n$ vertices with no edges.  When we see a positive example $x$, we add to $h$ every edge in $x$.  The same proof carries through.  At all times, $h$ is a subgraph of the true graph $G$, we define $p(e)$ as the probability of &lt;em&gt;not&lt;/em&gt; adding an edge to $h$ which is truly present in $G$.  Since we only ever add edges we are certain are in $G$, $p(e)$ for any $e\notin G$ is zero.  A &lt;em&gt;bad edge&lt;/em&gt; is now one with probability greater than $\frac{\epsilon}{|E|}$ of not appearing in any positive example, and the proof of the sample complexity is identical.&lt;/p&gt;

&lt;p&gt;This situation works because whereas learning from supergraphs was like learning a monotone conjunction over positive instances of literals, learning from subgraphs is like learning from negative examples of literals.  We start with the hypothesis ‘‘$e_1$ is not in $G$ and $e_2$ is not in $G$ and …’’ and as we see examples, we remove these literals until we get as our hypothesis a conjunction corresponding to the edges we are confident are not in $G$.  From here, it is easy to see how to move back and forth between a hypothesis consisting of edges we think are not in a graph and a hypothesis consisting of edges we know are in the graph.   Hence graphs are also PAC-learnable from subgraphs.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>First Year, First Post</title>
   <link href="http://example.com/blog/2017/05/first-year-first-post.html"/>
   <updated>2017-05-22T00:00:00-04:00</updated>
   <id>http://example.com/blog/2017/05/first-year-first-post</id>
   <content type="html">&lt;p&gt;So I’m one academic year into grad school.  A lot has happened in the year since I graduated from Colby: I moved from Waterville to Philadelphia, I completed two semeseters of graduate level CS coursework with minimal undergrad training in the discipline, I’ve started working on some interesting research problems, and I’ve met some interesting people.  I’ve now tried three times to get a blog going, but I had very little time and energy this year to sit down, set up the site, and get some content out there.&lt;/p&gt;

&lt;p&gt;This blog will host a variety of post types.  There will be personal stuff (like this), some discussions of my research and papers by others which I find interesting, posts on math/cs education, and discussions of current events.  For ‘hot takes’ and irreverent commentary on pop culture and complaints about the minutiae of my life, I’d like to direct you to my Twitter account, where I go by the handle &lt;a href=&quot;https://twitter.com/zschutzy&quot;&gt;@zschutzy&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;A lot of people (both in and outside of my academic circle) ask me &lt;em&gt;what exactly it is that I do&lt;/em&gt;.  Admittedly, I’m still very early in the grad school process and there are no guarantees that what I do now will be what I do in five years or even five months, but here’s a quick rundown of the stuff that I spend sixty hours a week thinking about.&lt;/p&gt;

&lt;h4 style=&quot;color:#800000;&quot; id=&quot;1-computational-economics&quot;&gt;1. &lt;strong&gt;&lt;em&gt;Computational Economics&lt;/em&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;Economics is the study of how society, institutions, and individuals allocate scarse resources.  One way that this happens in the everyday world is people buying goods and services from companies.  There are only so many apples at the Whole Foods, and they choose to set their price at, like, $7 per pound because that’s what they’ve determined is the profit-maximizing price (for whatever reason).  What happens when we have multiple people and multiple goods AND the people have more complex desires than just wanting some apples?  What if Alice and Bob show up to the store and Alice wants an apple and an orange and Bob wants an apple and a pear, neither of them will settle for having only one of the fruits (all-or-none), and there is only one apple?  How do we decide who gets the apple?  What if there dozens or hundreds of people and produce items?  It turns out that deciding the best allocation of goods in these settings is actually a really difficult problem for computers to solve, so Computational Economists come up with methods for finding approximate solutions to these kinds of problems in various settings.&lt;/p&gt;

&lt;h4 style=&quot;color:#800000;&quot; id=&quot;2-statistical-learning-theory&quot;&gt;2. &lt;strong&gt;&lt;em&gt;Statistical Learning Theory&lt;/em&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;The topic of &lt;em&gt;machine learning&lt;/em&gt; is a hot one these days.  In general, machine learning is really just applied statistics.  There are a lot of different algorithms for machine learning and people who use these in applied settings spend a lot of time thinking about which kinds of algorithms are good at their task at hand.  On the theoretical side, we are often interested in &lt;em&gt;mathematically proving&lt;/em&gt; limitations and capabilities of machine learning.  This area deals with questions like &lt;em&gt;how much data do I need for my model to be ‘good’?&lt;/em&gt; As it turns out, there are some kinds of problems that are hard for computers to learn (given current assumptions about the computational complexity of problems like factoring integers or graph coloring).&lt;/p&gt;

&lt;h4 style=&quot;color:#800000;&quot; id=&quot;3-computing-for-the-social-sciences&quot;&gt;3. &lt;strong&gt;&lt;em&gt;Computing for the Social Sciences&lt;/em&gt;&lt;/strong&gt;&lt;/h4&gt;

&lt;p&gt;In the last 25 years or so, personal computers have become powerful and accessible enough to chug through and solve a variety of really big problems which are provably computationally difficult and large volumes of data have become available at the click of a button.  This opens up new doors for research on problems which affect the day-to-day lives of people and the governance of society.  The problem that I am currently working on is related to optimal drawing of congressional districts, which given access to voting records and Census data, is now something that can be rigorously studied.  Some of my peers are working on using computational techniques to improve health insurance markets, school choice and drawing school districts, and using crowdsourcing techniques for gathering data.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you have any questions, comments, corrections, or suggestions, leave me a comment here or send me an email.  Enjoy!&lt;/p&gt;
</content>
 </entry>
 
</feed>
