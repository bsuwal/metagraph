<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>Your Title - computer</title>
 <link href="http://example.com/tag/computer/index.xml" rel="self"/>
 <link href="http://example.com/tag/computer.html"/>
 <updated>2018-06-18T08:57:32-04:00</updated>
 <id>http://example.com/tag/computer.html</id>
 <author>
   <name>Author Here</name>
 </author>
 
 <entry>
   <title>GerryCamp Week 2</title>
   <link href="http://example.com/blog/2018/06/vrdi-week-2.html"/>
   <updated>2018-06-18T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/06/vrdi-week-2</id>
   <content type="html">&lt;p&gt;Okay, the second week of redistricting summer camp is over and &lt;em&gt;a lot&lt;/em&gt; has happened.  Our first week of independent project work has highlighted many of the challenges we will face as researchers over the remaining weeks as well as demonstrated proof-of-concept for several promising new areas to look at.  The research approach is very divide-and-conquer, so we all split into groups to work on various projects, but I tried to keep a proverbial thumb in each of the proverbial pies, so I can quickly recap what I worked on in each of these.&lt;/p&gt;

&lt;h3 id=&quot;network-science&quot;&gt;Network Science&lt;/h3&gt;
&lt;p&gt;This was the group I was officially assigned to.  If you imagine each congressional district as being built out of geographic cells (like towns, precincts, or counties), then you can extract a graph structure from each district by making a vertex for each cell and connecting two vertices with an edge when their cells are adjacent as geographies.  Our goal was to explore some of the properties of these graphs, and we accomplished two major tasks.  First, we actually built these graphs for each state and district in the US.  Given the, umm, difficulty of working with the spatial data from the Census, this wasn’t as simple as we hoped it would be.  Given that last week I promised you some pictures of redistricting in Maine, you can see this graph for the Evergreen State below.  Second, we looked at embedding demographic and social data in these graphs and looking for similarities and clusters beyond spatial adjacency.  Our thoughts are that if two nearby neighborhoods have extremely similar interests, it may make sense to try to put them into the same district, but a graph using only spatial adjacencies cannot capture this information.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot; src=&quot;../../../assets/images/maine_dual.png?raw=true&quot; /&gt; &lt;br /&gt;

	&lt;font size=&quot;3&quot;&gt; A graph representation of Maine's two congressional districts &lt;/font&gt;
&lt;/p&gt;
&lt;div style=&quot;text-align: right&quot;&gt; &amp;#9724; &lt;/div&gt;

&lt;h3 id=&quot;markov-chain-monte-carlo-rebuild-aka-rundmcmc&quot;&gt;Markov Chain Monte Carlo Rebuild a.k.a. RunDMCMC&lt;/h3&gt;
&lt;p&gt;So there’s this big piece of code made by some researchers which basically takes in a districting plan, makes a bunch of small random changes to it, then spits out the new plan.  If you want your districts to be optimized for something, such as compactness, you can ask the code to only give you new plans which are better than your original in terms of that something.  Pretty cool, right?  Well, the one problem is that the existing code is a single C++ file with 1300 lines of code.  This group worked on rewriting this code in Python to be more readable and useable.  Since the MCMC process requires running for a very large number of small random changes, code performance is incredibly critical, and this was a fun opportunity to review some graph algorithms and learn about some of the Python libraries’ implementations of them.&lt;/p&gt;

&lt;h3 id=&quot;graph-partitions&quot;&gt;Graph Partitions&lt;/h3&gt;
&lt;p&gt;If we envision a state as a graph, then drawing districts is just a graph partitioning problem.  Easy, right?  Well, not really.  There are a lot of really basic questions that we (nor anyone else, as far as we can tell) do not know how to answer.  If I give you a graph, can you tell me how many ways there are to split it into two connected pieces?  If I give you a graph, can you sample uniformly from the set of all cuts which separate it into two connected pieces?  Are either of these problems in some class of computational hardness? If you know, please tell me! Otherwise, these are the big questions we’re thinking about and are hoping to answer soon.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;Finally, what’s happening next week?  I’m working with the Spectral Methods group, which is super exciting because learning spectal graph theory is one of my goals for this summer.  In short, spectral approaches look at the eigenvalues of objects and functions, and next week I’ll hopefully feel comfortable enough to write a basic primer to explain how we can use them to work on redistricting.  For now, here’s a little teaser: a picture of Florida, with the districts colored according to the largest eigenvalue of the graph’s adjacency matrix.&lt;/p&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot; src=&quot;../../../assets/images/fl_eval.png?raw=true&quot; /&gt; &lt;br /&gt;

	&lt;font size=&quot;3&quot;&gt; The largest eigenvalue of the adjacency matrix of each of Florida's districts &lt;/font&gt;
&lt;/p&gt;
&lt;div style=&quot;text-align: right&quot;&gt; &amp;#9724; &lt;/div&gt;

</content>
 </entry>
 
 <entry>
   <title>GerryCamp Week 1</title>
   <link href="http://example.com/blog/2018/06/vrdi-week-1.html"/>
   <updated>2018-06-09T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/06/vrdi-week-1</id>
   <content type="html">&lt;p&gt;This summer I am working with the &lt;a href=&quot;sites.tufts.edu/mggg&quot;&gt;MGGG&lt;/a&gt; as a Graduate Fellow with the &lt;a href=&quot;gerrydata.org&quot;&gt;Voting Rights Data Institute&lt;/a&gt;, a.k.a. gerrymandering summer camp a.k.a. GerryCamp.  I’m writing these weekly posts both as a way of documenting my work and experience as well as a sort of proof-of-life for my friends and colleagues I (temporarily) abandoned in Philadelphia.&lt;/p&gt;

&lt;p&gt;This week mostly consisted of a battery of talks geared towards getting us all up to speed with the terminology, tools, and techniques used in redistricting as well as getting us familiar with the Boston (well, not &lt;em&gt;in&lt;/em&gt; Boston, but nearby) area, as our working spaces are split among the campuses of Tufts, MIT, and Harvard.  Everyone comes from a wide range of backgrounds, and it’s been a lot of fun helping people learn about topics they may not have seen before.&lt;/p&gt;

&lt;p&gt;There are 52 undergraduate and graduate students at the VRDI, so we are each going to become an expert in an assigned state’s (plus DC and Puerto Rico) electoral and redistricting issues.  I have the great state of Maine, so stay tuned for some more stuff about that.  I think I’m giving a brief talk next week about apportionment in Maine, so maybe I can organize that into a cohesive document with lots of pretty maps and pictures.  Maybe I’ll do a blog post where I vent some of my complaints about how the election data doesn’t line up nicely with the census data…&lt;/p&gt;

&lt;p&gt;Finally, some goals for next week.  A few of us are trying to prove some theorems about projective geometry and graph partitioning, and with some luck we’ll be able to write down some definitive proofs or disproofs of these statements.  One major roadblock on the analytic side of the redistricting problem is that the space of all possible redistrictings (partitions of a map into a given number of equipopulous and contiguous pieces) is really, really, really, really big, and we don’t have a great way to randomly choose some plan in that set.  Something I’d like to do this week is either give a good algorithm to do so or prove that no good algorithm exists.&lt;/p&gt;

</content>
 </entry>
 
 <entry>
   <title>Strategic Classification from Revealed Preferences</title>
   <link href="http://example.com/blog/2018/04/strat-class-ec.html"/>
   <updated>2018-04-23T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/04/strat-class-ec</id>
   <content type="html">&lt;p&gt;Our paper &lt;em&gt;Strategic classification from revealed preferences&lt;/em&gt; was just accepted at &lt;a href=&quot;http://www.sigecom.org/ec18/&quot;&gt;EC ‘18&lt;/a&gt;!  Not only is it exciting that our work is being published, but this is also &lt;em&gt;my first publication&lt;/em&gt; which, I think, officially makes me &lt;em&gt;an academic&lt;/em&gt;.  At the very least, my &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s_number&quot;&gt;Erdős number&lt;/a&gt; is &lt;strong&gt;4&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Our paper examines a setting in which &lt;em&gt;self-interested agents&lt;/em&gt; interact with a &lt;em&gt;machine learning model&lt;/em&gt;.  Imagine you are designing a filter for an email system.  There are two kinds of emails that might be sent: legitimate ‘non-spam’ messages which you want to pass through the filter, and spam messages which you would like to block.  The challenge comes from how spammers and non-spammers respond differently to the filter system.  A non-spammer does not think about her message as being caught by a filter, so she does not carefully craft her emails to skirt your system.  In other words, she sends exactly the message she would like to send.  Spammers, on the other hand, are aware of the filtering system and have to tweak their emails to evade your filter.  The spammer has a message he would &lt;em&gt;like&lt;/em&gt; to send, but he may have to modify it in order for it to pass through the filter.  We model this change as incuring a cost to the spammers.&lt;/p&gt;

&lt;p&gt;We consider this problem in an &lt;em&gt;online&lt;/em&gt; setting.  At each time step, you publish your spam filter, and you observe whether you correctly or incorrectly labelled a message sent to your system.  You then get to adjust your filter.  At the surface, this problem is really, really, really hard to solve.  We only get to see the messages the spammers actually &lt;em&gt;send&lt;/em&gt;, rather than the ones they &lt;em&gt;wanted to send&lt;/em&gt;, and we don’t get to see the cost function a spammer used to inform his best-response to our filter.  At the core, we want to deploy the spam filter which minimizes our classification error &lt;em&gt;subject to the spammers trying to maximize our error on their examples&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In the paper we do two things: first, we demonstrate sufficient conditions for the optimization problem of choosing the best spam filter to be convex.  Second, we present a modification of an existing optimization algorithm which gives us a &lt;em&gt;no-regret&lt;/em&gt; online learning algorithm to deploy a sequence of filters.  The paper is pretty technical, but if you’re interested, you can check out a preprint from the fall &lt;a href=&quot;https://arxiv.org/abs/1710.07887&quot;&gt;here, on the arXiv&lt;/a&gt;.  For a precise, but less technical overview, we have a three-page ‘extended abstract’ &lt;a href=&quot;http://zachschutzman.com/assets/papers/stratclass_nips.pdf&quot;&gt;here&lt;/a&gt;, which appeared at the &lt;a href=&quot;http://www.cs.cmu.edu/~nhaghtal/mlstrat/&quot;&gt;Workshop on Learning in the Presence of Strategic Behavior&lt;/a&gt; at NIPS ‘17.&lt;/p&gt;
</content>
 </entry>
 
</feed>
