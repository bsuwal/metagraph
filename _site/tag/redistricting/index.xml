<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>Your Title - redistricting</title>
 <link href="http://example.com/tag/redistricting/index.xml" rel="self"/>
 <link href="http://example.com/tag/redistricting.html"/>
 <updated>2018-08-01T01:34:41-04:00</updated>
 <id>http://example.com/tag/redistricting.html</id>
 <author>
   <name>Author Here</name>
 </author>
 
 <entry>
   <title>GerryCamp Week 4</title>
   <link href="http://example.com/blog/2018/06/vrdi-week-4.html"/>
   <updated>2018-06-30T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/06/vrdi-week-4</id>
   <content type="html">Week 4 is done! We've only got two weeks left in the main program and things are starting to come together in a way that's really, really exciting.  This week I worked with the group building the Markov chain Monte Carlo (MCMC) simulator. Briefly, an MCMC sampler solves the problem of 'we know how to draw from distribution A but what we'd really like to do is draw from distribution B'.  In redistricting, we'd like to be able to generate a whole lot of legal districting plans in order to compare a proposed map to a random ensemble to, for example, determine if the proposed map treats a minority group unfairly.  Unfortunately, we don't know how to efficiently sample from that distribution.  What we can do efficiently is start with some plan and randomly mutate it for a while.  A Markov chain has a *mixing time*, which basically is the number of random mutations you have to make before your current state tells you nothing about where you started.  For example, if I start with the five letter string 'apple' and my Markov chain randomly changes one letter at a time, after about 15 steps my string might be 'udgxh' and if I just hand you that string, you probably can't meaningfully tell whether my initial string was 'apple'  or 'elbow' or 'zebra', so we say that this Markov chain mixes quickly.  

One major obstruction in the use of MCMC methods in redistricting is that we don't really know how fast the chain mixes.  Since there are so many constraints to obey, including equal population, contiguity, and compactness, there are relatively few moves (with respect to the number of allowable plans) the Markov chain can take at any time, so even if you run the chain for a few thousand or million steps, the resulting map doesn't look extremely different from the original.  This doesn't mean that the method is useless or hopeless, however.  Jon Mattingly and his team used an MCMC method on maps of North Carolina and were able to show that even though the Markov chain can only explore plans somewhat similar to the original, that the original scored much worse on factors like compactness and Voting Rights Act compliance than extremely similar maps, which is compelling evidence of the original map being a gerrymander.

As we've now passed the midway point of the five weeks of projects, we're going to pivot towards bringing various projects to a place where we can walk away from them and the 2019 iteration of the VRDI won't have to start from scratch.  This week I think I'll be working with someone on building a webapp to visualize and interact with the output of the MCMC code and getting back to the spectral stuff from last week.
</content>
 </entry>
 
 <entry>
   <title>GerryCamp Week 3</title>
   <link href="http://example.com/blog/2018/06/vrdi-week-3.html"/>
   <updated>2018-06-25T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/06/vrdi-week-3</id>
   <content type="html">Week three is done (and I guess week four has started, but I'm slow...) and I worked on All Things Spectral.  In short, to any districting plan, either as a collection of shapes or an abstract graph, we can associate a certain operator called the Laplacian which measures how geometrically similar the neighborhood of each point is to that of nearby points.  These operators have eigenfunctions in the geometric case and eigenvectors in the graph case, and the *spectrum* tell you something about the shape of the original object.  This week, I looked at a lot of eigenvalues.  I'd like to write up a little primer on these spectral methods in more detail, but I'd like to get this post up without any further delay.

As in much of research mathematics, a lot of time was spent on things that didn't work.  In reality, it's kind of surprising that we managed to find anything interesting at all in the brief four day work week.  What ended up being kind of remarkable were the *normalized laplacian eigenvalues* of the graphs representing districting plans.  In the short video below, there is a graph of the 200 smallest eigenvalues for various plans above an animated map of North Carolina.  The black curve is the eigenvalues for the graph representing the entire state, the blue curve for the current (115th Congress) districting plan, the green curve for the districting plan which was in place in 2012 before being invalidated by the courts, and the red curve for the plan seen in the animation in the lower frame, which is generated by a random MCMC process.  You can see that as the MCMC plan becomes more and more spider-y, the red curve moves downward, suggesting that this does measure something about district compactness!

&lt;p align=&quot;center&quot;&gt;
&lt;video src=&quot;../../../assets/images/nc_mcmc.mp4&quot; width=&quot;600 &quot; height=&quot;600&quot; controls preload&gt;&lt;/video&gt;

	
&lt;/p&gt;
&lt;div style=&quot;text-align: right&quot;&gt; &amp;#9724; &lt;/div&gt;





</content>
 </entry>
 
 <entry>
   <title>GerryCamp Week 2</title>
   <link href="http://example.com/blog/2018/06/vrdi-week-2.html"/>
   <updated>2018-06-18T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/06/vrdi-week-2</id>
   <content type="html">Okay, the second week of redistricting summer camp is over and *a lot* has happened.  Our first week of independent project work has highlighted many of the challenges we will face as researchers over the remaining weeks as well as demonstrated proof-of-concept for several promising new areas to look at.  The research approach is very divide-and-conquer, so we all split into groups to work on various projects, but I tried to keep a proverbial thumb in each of the proverbial pies, so I can quickly recap what I worked on in each of these.

### Network Science
This was the group I was officially assigned to.  If you imagine each congressional district as being built out of geographic cells (like towns, precincts, or counties), then you can extract a graph structure from each district by making a vertex for each cell and connecting two vertices with an edge when their cells are adjacent as geographies.  Our goal was to explore some of the properties of these graphs, and we accomplished two major tasks.  First, we actually built these graphs for each state and district in the US.  Given the, umm, difficulty of working with the spatial data from the Census, this wasn't as simple as we hoped it would be.  Given that last week I promised you some pictures of redistricting in Maine, you can see this graph for the Evergreen State below.  Second, we looked at embedding demographic and social data in these graphs and looking for similarities and clusters beyond spatial adjacency.  Our thoughts are that if two nearby neighborhoods have extremely similar interests, it may make sense to try to put them into the same district, but a graph using only spatial adjacencies cannot capture this information.  

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot;  src=&quot;../../../assets/images/maine_dual.png?raw=true&quot;&gt; &lt;br /&gt;

	&lt;font size=&quot;3&quot;&gt; A graph representation of Maine's two congressional districts &lt;/font&gt;
&lt;/p&gt;
&lt;div style=&quot;text-align: right&quot;&gt; &amp;#9724; &lt;/div&gt;

### Markov Chain Monte Carlo Rebuild a.k.a. RunDMCMC
So there's this big piece of code made by some researchers which basically takes in a districting plan, makes a bunch of small random changes to it, then spits out the new plan.  If you want your districts to be optimized for something, such as compactness, you can ask the code to only give you new plans which are better than your original in terms of that something.  Pretty cool, right?  Well, the one problem is that the existing code is a single C++ file with 1300 lines of code.  This group worked on rewriting this code in Python to be more readable and useable.  Since the MCMC process requires running for a very large number of small random changes, code performance is incredibly critical, and this was a fun opportunity to review some graph algorithms and learn about some of the Python libraries' implementations of them.

### Graph Partitions
If we envision a state as a graph, then drawing districts is just a graph partitioning problem.  Easy, right?  Well, not really.  There are a lot of really basic questions that we (nor anyone else, as far as we can tell) do not know how to answer.  If I give you a graph, can you tell me how many ways there are to split it into two connected pieces?  If I give you a graph, can you sample uniformly from the set of all cuts which separate it into two connected pieces?  Are either of these problems in some class of computational hardness? If you know, please tell me! Otherwise, these are the big questions we're thinking about and are hoping to answer soon.

----
Finally, what's happening next week?  I'm working with the Spectral Methods group, which is super exciting because learning spectal graph theory is one of my goals for this summer.  In short, spectral approaches look at the eigenvalues of objects and functions, and next week I'll hopefully feel comfortable enough to write a basic primer to explain how we can use them to work on redistricting.  For now, here's a little teaser: a picture of Florida, with the districts colored according to the largest eigenvalue of the graph's adjacency matrix.

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;300&quot;  src=&quot;../../../assets/images/fl_eval.png?raw=true&quot;&gt; &lt;br /&gt;

	&lt;font size=&quot;3&quot;&gt; The largest eigenvalue of the adjacency matrix of each of Florida's districts &lt;/font&gt;
&lt;/p&gt;
&lt;div style=&quot;text-align: right&quot;&gt; &amp;#9724; &lt;/div&gt;





</content>
 </entry>
 
 <entry>
   <title>GerryCamp Week 1</title>
   <link href="http://example.com/blog/2018/06/vrdi-week-1.html"/>
   <updated>2018-06-09T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/06/vrdi-week-1</id>
   <content type="html">This summer I am working with the [MGGG](sites.tufts.edu/mggg) as a Graduate Fellow with the [Voting Rights Data Institute](gerrydata.org), a.k.a. gerrymandering summer camp a.k.a. GerryCamp.  I'm writing these weekly posts both as a way of documenting my work and experience as well as a sort of proof-of-life for my friends and colleagues I (temporarily) abandoned in Philadelphia.

This week mostly consisted of a battery of talks geared towards getting us all up to speed with the terminology, tools, and techniques used in redistricting as well as getting us familiar with the Boston (well, not *in* Boston, but nearby) area, as our working spaces are split among the campuses of Tufts, MIT, and Harvard.  Everyone comes from a wide range of backgrounds, and it's been a lot of fun helping people learn about topics they may not have seen before.

There are 52 undergraduate and graduate students at the VRDI, so we are each going to become an expert in an assigned state's (plus DC and Puerto Rico) electoral and redistricting issues.  I have the great state of Maine, so stay tuned for some more stuff about that.  I think I'm giving a brief talk next week about apportionment in Maine, so maybe I can organize that into a cohesive document with lots of pretty maps and pictures.  Maybe I'll do a blog post where I vent some of my complaints about how the election data doesn't line up nicely with the census data...

Finally, some goals for next week.  A few of us are trying to prove some theorems about projective geometry and graph partitioning, and with some luck we'll be able to write down some definitive proofs or disproofs of these statements.  One major roadblock on the analytic side of the redistricting problem is that the space of all possible redistrictings (partitions of a map into a given number of equipopulous and contiguous pieces) is really, really, really, really big, and we don't have a great way to randomly choose some plan in that set.  Something I'd like to do this week is either give a good algorithm to do so or prove that no good algorithm exists.

</content>
 </entry>
 
</feed>
