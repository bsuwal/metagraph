<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
 <title>Your Title - machine</title>
 <link href="http://example.com/tag/machine/index.xml" rel="self"/>
 <link href="http://example.com/tag/machine.html"/>
 <updated>2018-06-25T12:47:11-04:00</updated>
 <id>http://example.com/tag/machine.html</id>
 <author>
   <name>Author Here</name>
 </author>
 
 <entry>
   <title>Strategic Classification from Revealed Preferences</title>
   <link href="http://example.com/blog/2018/04/strat-class-ec.html"/>
   <updated>2018-04-23T00:00:00-04:00</updated>
   <id>http://example.com/blog/2018/04/strat-class-ec</id>
   <content type="html">&lt;p&gt;Our paper &lt;em&gt;Strategic classification from revealed preferences&lt;/em&gt; was just accepted at &lt;a href=&quot;http://www.sigecom.org/ec18/&quot;&gt;EC ‘18&lt;/a&gt;!  Not only is it exciting that our work is being published, but this is also &lt;em&gt;my first publication&lt;/em&gt; which, I think, officially makes me &lt;em&gt;an academic&lt;/em&gt;.  At the very least, my &lt;a href=&quot;https://en.wikipedia.org/wiki/Erd%C5%91s_number&quot;&gt;Erdős number&lt;/a&gt; is &lt;strong&gt;4&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;Our paper examines a setting in which &lt;em&gt;self-interested agents&lt;/em&gt; interact with a &lt;em&gt;machine learning model&lt;/em&gt;.  Imagine you are designing a filter for an email system.  There are two kinds of emails that might be sent: legitimate ‘non-spam’ messages which you want to pass through the filter, and spam messages which you would like to block.  The challenge comes from how spammers and non-spammers respond differently to the filter system.  A non-spammer does not think about her message as being caught by a filter, so she does not carefully craft her emails to skirt your system.  In other words, she sends exactly the message she would like to send.  Spammers, on the other hand, are aware of the filtering system and have to tweak their emails to evade your filter.  The spammer has a message he would &lt;em&gt;like&lt;/em&gt; to send, but he may have to modify it in order for it to pass through the filter.  We model this change as incuring a cost to the spammers.&lt;/p&gt;

&lt;p&gt;We consider this problem in an &lt;em&gt;online&lt;/em&gt; setting.  At each time step, you publish your spam filter, and you observe whether you correctly or incorrectly labelled a message sent to your system.  You then get to adjust your filter.  At the surface, this problem is really, really, really hard to solve.  We only get to see the messages the spammers actually &lt;em&gt;send&lt;/em&gt;, rather than the ones they &lt;em&gt;wanted to send&lt;/em&gt;, and we don’t get to see the cost function a spammer used to inform his best-response to our filter.  At the core, we want to deploy the spam filter which minimizes our classification error &lt;em&gt;subject to the spammers trying to maximize our error on their examples&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;In the paper we do two things: first, we demonstrate sufficient conditions for the optimization problem of choosing the best spam filter to be convex.  Second, we present a modification of an existing optimization algorithm which gives us a &lt;em&gt;no-regret&lt;/em&gt; online learning algorithm to deploy a sequence of filters.  The paper is pretty technical, but if you’re interested, you can check out a preprint from the fall &lt;a href=&quot;https://arxiv.org/abs/1710.07887&quot;&gt;here, on the arXiv&lt;/a&gt;.  For a precise, but less technical overview, we have a three-page ‘extended abstract’ &lt;a href=&quot;http://zachschutzman.com/assets/papers/stratclass_nips.pdf&quot;&gt;here&lt;/a&gt;, which appeared at the &lt;a href=&quot;http://www.cs.cmu.edu/~nhaghtal/mlstrat/&quot;&gt;Workshop on Learning in the Presence of Strategic Behavior&lt;/a&gt; at NIPS ‘17.&lt;/p&gt;
</content>
 </entry>
 
</feed>
